{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc854e34",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "The term linar regression refers to using a linear model to represent the relationship between a set of independent variables and a dependent variable.\n",
    "\n",
    "### y = ax1 + bx2 + cx3 + d\n",
    "\n",
    "The above formula is example linear model which produces output y (dependent variable) based onthe linear combination of independent variable x1, x2, x3. The coefficients a,b,c and intercept d determine the model's fit.\n",
    "\n",
    "\n",
    "The simplest form of linear regression is called least squares regression. This strategy produces a regression model which is a linear combination of the independent variables, that minimizes the sum of squared residuals between the model's prediction and actual values for the dependent variable.\n",
    "\n",
    "### reg = linear_model.LinearRegression()\n",
    "###  reg.fit(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfa3cce",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "#### not only minimize the sum of squared residuals but to do this with coefficients as small as possible\n",
    "\n",
    "While ordinary least squares regression is a good way to fit a linear model onto a dataset, it relies on the fact that the dataset's features are each independent, i.e. uncorrelated. When many of the dataset features are linearly correlated, e.g. if a dataset has multiple features depicting the same price in different currencies, it makes the least squares regression highly sensitive to noise in the data.\n",
    "\n",
    "Because real life data tends to have noise, and will often have some linearly correlated features in the dataset, we combat this by performing regularization. Fir ordinary least squares regression, the goal is to find the weights(codefficients) for the lnear model the minimize  the sum of squared residuals\n",
    "\n",
    "### sigma(x_i . w - y_i)^2\n",
    "\n",
    "where each x_i represents a data observation and y+i is the corresponding label.\n",
    "\n",
    "For regularization, the goal is to not only minimize the sum of squared reduals , but to do this with coefficients as small as possible. The smaller the coefficients, the less susceptible they are to random noise in the data. The most commonly use form of regularization is ridge regularization.\n",
    "\n",
    "With redge regularizatoin the is now to find the weights that minimize the following quantity:\n",
    "\n",
    "### alpha *|| w||_2 ^ 2 + sigma( x_i .w - y_i)^2\n",
    "\n",
    "where alpha is a non-negative real number hyperparameter and ||w||_2 represenst L2 norm of the weights. The additional alpha ||w||_2 ^ 2 is referred to as the penalty term, since it penalizes larger weight values. Larger quantities of alpha will put greater emphasiss on the penalty term, forcing the model to have even smaller weight values.\n",
    "\n",
    "\n",
    "### reg = linear_model.Ridge(alpha=0.1)\n",
    "### reg.fit(data, labels)\n",
    "\n",
    "Rather than manually choosing a values (alpha) we can use cross-validation to hel us choose the optimal alpha from a list of values.\n",
    "\n",
    "RidgeCV: ridge regression using cross validation\n",
    "\n",
    "### reg = linear_model.RidgeCV(alphas=alphas)\n",
    "### reg.fit(pizza_data, pizza_prices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088e21b",
   "metadata": {},
   "source": [
    "# LASSO regression \n",
    "\n",
    "While ridge regularization use an L2 norm penality term, LASSO uses L1 norm for the weight penality term. Specifically, LASSO regularization will find the optimal weights to minimize the following quantity:\n",
    "\n",
    "### alpha ||w||_1 + sigma ( x_i .w - y_i)^2\n",
    "where || w ||_1 represents L1 norm of the weights.\n",
    "\n",
    "LASSO regression tends to prefer linear models with fewer parameter values. This means that it will likely zero-out some of the weight coefficients. This means that it will likelt zero-out some of the weight coefficients. This reduces the number of features that the model is actually dependent on ( since some of the coefficients will now be 0), which can be benificial when some features are completely irrevelent or duplicates of other features.\n",
    "\n",
    "### reg = linear_model.Lasso(alpha=0.1)\n",
    "### reg.fit(data, labels)\n",
    "\n",
    "There is also version with cross-validation LassoCV same as ridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3dd07",
   "metadata": {},
   "source": [
    "# Bayesian Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d4b99",
   "metadata": {},
   "source": [
    "A major assumption that the K-means clustering algorithm makes is that the dataset consists of sperical (i.e circular) clusters. With this assumption, the K-means algorithm will create clusters of data observations that are circular around the centroids. However, real life data often does not contain spherical clusters, meaning that K-means clustering might end up producing inaccurate clusters due to its assumption.\n",
    "\n",
    "An alternative to K-means clustering is hierarchical clustering. Heirarchical clustering allows us to cluster any type of data, since it doesn't make any assumptions about the data or clusters.\n",
    "\n",
    "There are two approaches to hierarchical clustering: bottom-up(divisive) and top-down(agglomerative). The divisive approach initially treats all the data as a single cluster, then repeatedly splits it into smaller clusters until we reach the desired number of clusters.\n",
    "The agglomerative approach initially treats each data observation as its own cluster, then repeatedly merges the two most similar clusters until we reach the desired number of clusters. In practice, the agglomerativ approach is more commonly used due to better algorithms for the approach. Therefore, we'll focus on using agglomerative clustering in this chapter.\n",
    "\n",
    "\n",
    "MEAN Shift clustering:\n",
    "\n",
    "Each of thec lustering algorithms we've used so far require us to pass in the number of clusters we want, or have a good guess for the number of clusters. However, if we don't have a good idea of what the actual number of clusters for the dataset should be, there exist algorithms that can automatically choose the number of clusters for us.\n",
    "\n",
    "One such algorithm is the mean shift clustering algorithm. Like the K-means clustering algorithm, the mean shift algorithm is based on finding cluster centroids. since we don't provide the number of clusters, the algorithm will look for blobs in the data that can be potential candidates for clusters. Using these \"blobs\", the algorithm finds as number of candidate centroids. It then removes the candidates that are basically duplicates of others to form the final set of centroids. The final set of centroids determines the number of clusters as well as the dataset cluster assignmenets (data observations are assigned to the nearest centroid).\n",
    "\n",
    "Since mean shift is a centroid-based algorithm, the MeanShift object contains the cluster_centers_ attribute (the final centroids) and the predict function.\n",
    "\n",
    "\n",
    "DBSCAN:\n",
    "\t\"Clustering by density\":\n",
    "\t\tThe mean shift clustering algorithm in the previous cahpter usually performs sufficiently well and can choose a reasonable number of clusters. However, it is not very scalable due to computation time and still makes the assumption that clusters have a \"blob\"- like shape (although this assumption is not as strong as the one made by K-means).\n",
    "\t\tAnother clustering algorithm that also automatically chooses the number of clusters is DBSCAN. DBSCAN clusters data by finding dense regions in the dataset. Regions in the dataset with many closely packed data observations are considered high-density regions, while regions with sparese data are considered low-density regions.\n",
    "\t\tThe DBSCAN algorithm treats high-density regions as clusters in the dataset, and low-density regions as the area between clusters (so observations in the low-density are treated as noise and not placed in a cluster).\n",
    "\t\tHigh-density regions are defined by core samples, which are just data observations with many neighbors. Each cluster consists of several core samples and all the observations that are neighbors to a core sample.\n",
    "\t\tUnlike the mean shift algorithm, the DBSCAN algorithm is both highly scalable and makes no assumptions about the underlying shape of clusters in the dataset.\n",
    "\t\t\n",
    "\t\"Neighbors and core samples\"\n",
    "\t\tThe exact definition of \"neighbor\" and \"core sample\" depends on what we want in our clusters. We specify the maximum distance, (epsilon), between two data observation that are considered neighbors. Smaller distances result in smaller and more tightly packed clusters. We also specify the minimum number of points in the neighborhood of a data observation for the observation to be considered a core sample (the neighborhood consists of the data observation and all its neighbors).\n",
    "\t\t\n",
    "EVALUATION METRICS:\n",
    "\tWhen we don't have access to any true cluster assignments(labels), the best we can do to evaluate clusters is to just take a look at them and see if they make sense with respects to the dataset and domain. However, if we do have acess to the true cluster labels for the data obeservations, we can apply a number of metrics to evaluate or clustering algorithm.\n",
    "\t\n",
    "\tOne popular evaluation metric is the adjusted Rand index. The regular Rand index gives a measurement of similarity between the true clustering assignments(true lables) and the predicted clustering assignments (predicted labels). The adjusted Rand index(ARI) is a corrected-for-chance version of the regular one, meaning that the score is adjusted so that random clustering assignments will not have a good score.\n",
    "The ARI value ranges from -1 to 1, inclusive. Negative scores represent bat labelings, random labelings wil get a score near 0 and perfect labelings get a score of 1.\n",
    "\t\t\n",
    "\t\tAnother common clustering evaluation metric is adjusted mutual information (AMI). It is implemented in scikit-learn with adjusted_mutual_info_score function (also part of the cluster module).\n",
    "\t\t\n",
    "\t\tThe ARI and AMI metrics are very similar. They both assign a score of 1 to perfect labelings, a score near 0.0 to random labelings and negative scores to poor labelings.\n",
    "\t\t\n",
    "\t\tA general rule of thumb of when to use which: ARI is used when the true clusters are large and approximaterly equal sized, while AMI is used when the true clusters are unbalanced in size and there exist small clusters.\n",
    "\t\t\n",
    "\t\t\n",
    "AGGLORMERATIVE FEATURE CLUSTERING:\n",
    "\tIn the Data Preprocessing section, we used PCA to perform feature dimensionality reduction on datasets. We can also perform feature dimensionality reduction using agglomerative clustering. By merging common features into clusters, we reduce the number of total features while still maintaining most of the original information from the dataset.\n",
    "\t(scikit-learn: FeatureAgglormerative)\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
